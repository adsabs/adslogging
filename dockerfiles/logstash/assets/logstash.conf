input { 
    # used for testing
    # send stuff to logstash via netcat: `echo "foo" | nc localhost 3333`
    tcp { type => "ads" port => 3333 tags => ["beer","apache","access"] }
    tcp { type => "ads" port => 3334 tags => ["classic","apache","access"] }
    tcp { type => "ads" port => 3335 tags => ["bit_server"] }
    tcp { type => "ads" port => 3336 tags => ["data_server"] }
    tcp { type => "ads" port => 3337 tags => ["apache","error"] }
    tcp { type => "ads" port => 3338 format => "json_event" }

    # these redis lists are written to directly by the app
    redis {
        host => 'localhost' 
        port => 6378
        data_type => 'list' 
        type => 'beer-abs' 
        key => 'adsabs:abs'
        tags => ['beer','abs']
    }
    redis {
        host => 'localhost' 
        port => 6378
        data_type => 'list'
        type => 'beer-api' 
        key => 'adsabs:api'
        tags => ['beer','api']
    }
    redis {
        host => 'localhost'
        port => 6378
        data_type => 'list'
        type => 'beer-search' 
        key => 'adsabs:search'
        tags => ['beer','search']
    }
    # this list is populated by the beaver daemon reading from log files
    redis {
        host => 'localhost' 
        port => 6378
        data_type => 'list'
        key => 'adsabs:logs'
        tags => ['beer','logs','apache']
        type => "_unset" # shippers are responsible for setting event 'type'
    }
    # this list is populated by either beaver or skidder (for backfilling)
    redis {
        host => 'localhost'
        port => 6378
        data_type => 'list'
        key => 'classic:logs'
        threads => 4
        tags => ['classic','logs','apache']
        type => "_unset" # shippers are responsible for setting event 'type'
    }
    redis {
        host => 'localhost' 
        port => 6378
        data_type => 'list'
        key => 'solr:logs'
        tags => ['solr','logs']
        type => "_unset" # shippers are responsible for setting event 'type'
    }
}

filter {
    # beer apache access log
    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["beer","apache","access"]
        match => ["message", "%{COMBINDEDWITHRESPTIME}"]
    }

    # classic apache access log
    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["classic","apache","access"]
        match => ["message", "%{COMBINEDWITHCOOKIE}"]
    }

    # should handle dates for classic & beer apache access
    date {
        tags => ["apache","access"]
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }

    # should handle both apache error logs
    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["apache","error"]
        match => ["message", "%{GENERICAPACHEERROR}"]
    }

    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["apache","access","classic"]
        match => ["path", "/abs/%{BIBCODE:bibcode}"]
        tag_on_failure => false
    }

    multiline {
        tags => ["solr"]
        pattern => "^\s+"
        negate => false
        what => "previous"
    }

    grep {
        match => ["message", "Exception"]
        drop => false
        add_tag => ["exception","multiline"]
    }

    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["solr","exception"]
        match => ["message", "(?m)%{SOLREXCEPTION}"]
    }

    # should handle dates for classic & beer apache access
    date {
        tags => ["solr"]
        match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }

    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["solr"]
        exclude_tags => ["exception"]
        match => ["message", "%{SOLRLOG}"]
    }

    # remove quotes from some values
    mutate {
        tags => ["apache","access"]
        gsub => [
            "referrer", "\"", "",
            "cookie", "\"", "",
            "agent", "\"", "",
            "proxy", "\"", ""
        ]
    }

    urldecode {
        tags => ["apache","access"]
        all_fields => true
    }

    kv {
        tags => ["apache","access"]
        source => ["qstring"]
        prefix => "qstring__"
        field_split => "?&"
        value_split => "="
        add_tag => ["kv_applied"]
    }
    kv {
        tags => ["apache","access"]
        source => ["cookie"]
        prefix => "cookie__"
        field_split => " "
        value_split => "="
        trim => ";"
        add_tag => ["kv_applied"]
    }

    kv {
        tags => ["solr"]
        exclude_tags => ["exception"]
        source => ["solr_msg"]
        field_split => " "
        value_split => "="
        trim => "{}"
    }

    kv {
        tags => ["solr"]
        exclude_tags => ["exception"]
        source => ["params"]
        prefix => "param__"
        field_split => "&"
        value_split => "="
    }

#    ruby {
#        tags => ["kv_applied"]
#        code => "event['@fields'] = event['@fields'].delete_if { |key, value| key.to_s.length > 30 and (key.to_s.start_with?('cookie__') or key.to_s.start_with?('qstring__')) }"
#        add_tag => ["keys_truncated"]
#    }

    grep {
        tags => ["apache","access"]
        match => ["path", "/api/"]
        drop => false
        add_tag => ["api"]
    }

    grep {
        tags => ["solr"]
        exclude_tags => ["exception"]
        match => ["path", "\/ping$"]
        negate => true
    }

    grep {
        tags => ["solr"]
        exclude_tags => ["exception"]
        match => ["path", "\/select$"]
        add_tag => ["search"]
        drop => false
    }

########################################################################
# Next section relates to old data server logs
########################################################################
    # ads bit server log
#    grok {
#        patterns_dir => "/etc/logstash/patterns"
#        tags => ["bit_server"]
#        match => ["message", "%{ADSBITSERVER}"]
#        add_tag => ["stats"]
#    }
    # ads data server log
#    grok {
#        patterns_dir => "/etc/logstash/patterns"
#        tags => ["data_server"]
#        match => ["message", "%{ADSDATASERVER}"]
#        add_tag => ["stats"]
#    }

#    mutate {
#        tags => ["stats"]
#        gsub => [
#            "logdate", "\b(?<val>\d)\b", "0\k<val>",
#            "logdate", "\s+", " "
#        ]
#    }
#    date {
#        tags => ["stats"]
#        match => [ "logdate", "yyyy MM dd HH mm ss" ]
#    }

########################################################################

    # do a reverse lookup on the ip...
    # don't do this for classic (backfilled) logs as it's too slow
    dns {
        tags => ["beer"]
        action => "replace"
        reverse => ["client_addr"]
        add_tag => ["dns_reversed"]
    }
    # ... then check if it matches a bot or known "client" software
    grok {
        patterns_dir => "/etc/logstash/patterns"
        tags => ["dns_reversed"]
        match => ["client_addr", "%{BOTORCLIENT:bot_or_client}"]
        tag_on_failure => false
        add_tag => ["bot"]
    }
    useragent {
        tags => ["apache","access"]
        source => "agent"
        prefix => "useragent__"
    }

    # drop requests for static resources & javascript
    grep { 
        tags => ["beer", "apache"]
        match => ['path', "static"] 
        negate => true 
    }
    grep { 
        tags => ["beer", "apache"]
        match => ['path', "dynjs"] 
        negate => true 
    }

    # prune stuff to keep the ES index size from blowing up
    mutate {
        tags => ["apache","access"]
        remove => ["cookie","agent","qstring"]
    }
}

output {
    stdout { codec => rubydebug }
    elasticsearch_http {
        host => "localhost"
        manage_template => false
    }
}
