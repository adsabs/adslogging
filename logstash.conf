input { 
    # used for testing
    tcp { type => "ads" port => 3333 tags => ["beer","apache","access"] }
    tcp { type => "ads" port => 3334 tags => ["classic","apache","access"] }
    tcp { type => "ads" port => 3335 tags => ["bit_server"] }
    tcp { type => "ads" port => 3336 tags => ["data_server"] }
    tcp { type => "ads" port => 3337 tags => ["apache","error"] }
    tcp { type => "ads" port => 3338 format => "json_event" }

    # these redis lists are written to directly by the app
    redis {
        host => 'localhost' data_type => 'list' format => "json_event"
        type => 'beer-abs' 
        key => 'adsabs:abs'
        tags => ['beer','abs']
    }
    redis {
        host => 'localhost' data_type => 'list' format => "json_event"
        type => 'beer-api' 
        key => 'adsabs:api'
        tags => ['beer','api']
    }
    redis {
        host => 'localhost' data_type => 'list' format => "json_event"
        type => 'beer-search' 
        key => 'adsabs:search'
        tags => ['beer','search']
    }
    # this list is populated by the beaver daemon reading from log files
    redis {
        host => 'localhost' data_type => 'list' format => "json_event"
        type => 'beer-logs' 
        key => 'adsabs:adsx:logs'
        tags => ['beer','logs','apache','access']
    }
    redis {
        host => 'localhost' data_type => 'list' format => 'json_event'
        type => 'classic-logs' 
        key => 'skidder:classic:logs' threads => 4
        tags => ['classic','logs','apache','access']
    }
}

filter {
    # beer apache access log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["beer","apache","access"]
        match => ["@message", "%{COMBINDEDWITHRESPTIME}"]
    }

    # classic apache access log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["classic","apache","access"]
        match => ["@message", "%{COMBINEDWITHCOOKIE}"]
    }

    # should handle dates for classic & beer apache access
    date {
        tags => ["apache","access"]
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }

    # should handle both apache error logs
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["apache","error"]
        match => ["@message", "%{GENERICAPACHEERROR}"]
    }

    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["apache","access","classic"]
        match => ["path", "/abs/%{BIBCODE:bibcode}"]
        tag_on_failure => false
    }

    # remove quotes from some values
    mutate {
        tags => ["apache","access"]
        gsub => [
            "referrer", "\"", "",
            "cookie", "\"", "",
            "agent", "\"", "",
            "proxy", "\"", ""
        ]
    }

    urldecode {
        tags => ["apache","access"]
        all_fields => true
    }

    kv {
        tags => ["apache","access"]
        source => ["qstring"]
        prefix => "qstring__"
        field_split => "?&"
        value_split => "="
    }
    kv {
        tags => ["apache","access"]
        source => ["cookie"]
        prefix => "cookie__"
        field_split => " "
        value_split => "="
        trim => ";"
    }

    grep {
        tags => ["apache","access"]
        match => ["path", "/api/"]
        drop => false
        add_tag => ["api"]
    }

########################################################################
# Next section relates to old data server logs
########################################################################
    # ads bit server log
#    grok {
#        patterns_dir => "./adslogging/logstash_patterns"
#        tags => ["bit_server"]
#        match => ["@message", "%{ADSBITSERVER}"]
#        add_tag => ["stats"]
#    }
    # ads data server log
#    grok {
#        patterns_dir => "./adslogging/logstash_patterns"
#        tags => ["data_server"]
#        match => ["@message", "%{ADSDATASERVER}"]
#        add_tag => ["stats"]
#    }

#    mutate {
#        tags => ["stats"]
#        gsub => [
#            "logdate", "\b(?<val>\d)\b", "0\k<val>",
#            "logdate", "\s+", " "
#        ]
#    }
#    date {
#        tags => ["stats"]
#        match => [ "logdate", "yyyy MM dd HH mm ss" ]
#    }

########################################################################

    # do a reverse lookup on the ip...
    # don't do this for classic (backfilled) logs as it's too slow
    dns {
        tags => ["beer"]
        action => "replace"
        reverse => ["client_addr"]
        add_tag => ["dns_reversed"]
    }
    # ... then check if it matches a bot or known "client" software
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["dns_reversed"]
        match => ["client_addr", "%{BOTORCLIENT:bot_or_client}"]
        tag_on_failure => false
        add_tag => ["bot"]
    }
    useragent {
        tags => ["apache","access"]
        source => "agent"
        prefix => "useragent__"
    }

    # drop requests for static resources & javascript
    grep { 
        tags => ["beer", "apache"]
        match => ['path', "static"] 
        negate => true 
    }
    grep { 
        tags => ["beer", "apache"]
        match => ['path', "dynjs"] 
        negate => true 
    }

    # prune stuff to keep the ES index size from blowing up
    mutate {
        tags => ["apache","access"]
        remove => ["cookie","agent","qstring","@message"]
    }
}

output {
    stdout { debug => true }
    elasticsearch {
        host => "localhost"
        cluster => "logstash_es"
        node_name => "Node1"
        exclude_tags => ["metric"]
        max_inflight_requests => 500
    }
}
