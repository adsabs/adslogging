input { 
#    stdin { type => 'test' } 

    # used for testing
    tcp { type => "ads" port => 3333 tags => ["beer","apache","access"] }
    tcp { type => "ads" port => 3334 tags => ["classic","apache","access"] }
    tcp { type => "ads" port => 3335 tags => ["bit_server"] }
    tcp { type => "ads" port => 3336 tags => ["data_server"] }
    tcp { type => "ads" port => 3337 tags => ["apache","error"] }
    tcp { type => "ads" port => 3338 format => "json_event" }

    # these redis lists are written to directly by the app
    redis {
        type => 'ads' host => 'adsx' data_type => 'list' format => "json_event"
        key => 'adsabs:abs'
        tags => ['beer','abs']
    }
    redis {
        type => 'ads' host => 'adsx' data_type => 'list' format => "json_event"
        key => 'adsabs:api'
        tags => ['beer','api']
    }
    redis {
        type => 'ads' host => 'adsx' data_type => 'list' format => "json_event"
        key => 'adsabs:search'
        tags => ['beer','search']
    }
    # this list is populated by the beaver daemon reading from log files
    redis {
        type => 'ads' host => 'adsx' data_type => 'list' format => "json_event"
        key => 'adsabs:adsx:logs'
    }
    redis {
        type => 'ads' host => 'adsx' data_type => 'list' format => 'json_event'
        key => 'skidder:classic' threads => 4
    }
}

filter {
    # beer apache access log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["beer","apache","access"]
        match => ["@message", "%{COMBINDEDWITHRESPTIME}"]
    }

    # classic apache access log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["classic","apache","access"]
        match => ["@message", "%{COMBINEDWITHCOOKIE}"]
    }

    # should handle dates for classic & beer apache access
    date {
        tags => ["apache","access"]
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }

    # should handle both apache error logs
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["apache","error"]
        match => ["@message", "%{GENERICAPACHEERROR}"]
    }

    # remove quotes from some values
    mutate {
        tags => ["apache","access"]
        gsub => [
            "referrer", "\"", "",
            "cookie", "\"", "",
            "agent", "\"", "",
            "proxy", "\"", ""
        ]
    }

    urldecode {
        tags => ["apache","access"]
        all_fields => true
    }

    kv {
        tags => ["apache","access"]
        source => ["qstring"]
        prefix => "qstring__"
        field_split => "?&"
        value_split => "="
    }
    kv {
        tags => ["apache","access"]
        source => ["cookie"]
        prefix => "cookie__"
        field_split => " "
        value_split => "="
        trim => ";"
    }
#    grep {
#        tags => ["apache","access"]
#        match => ["cookie", "[^-]+"] # value contains more than just "-"
#        drop => false
#        add_tag => "has_cookie"
#    }
	
#    mutate {
#        tags => ["apache","access","has_cookie"]
#        add_field => ["NASA_ADS_ID","%{cookie_vals.NASA_ADS_ID}"]
#        add_field => ["NASA_ADS_LABS_ID","%{cookie_vals.NASA_ADS_LABS_ID}"]
#        add_field => ["NASA_ADSABS2_ID","%{cookie_vals.NASA_ADSABS2_ID}"]
#    }
#
    grep {
        tags => ["apache","access"]
        match => ["path", "/api/"]
        drop => false
        add_tag => ["api"]
    }

    # ads bit server log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["bit_server"]
        match => ["@message", "%{ADSBITSERVER}"]
        add_tag => ["stats"]
    }
    # ads data server log
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["data_server"]
        match => ["@message", "%{ADSDATASERVER}"]
        add_tag => ["stats"]
    }
    mutate {
        tags => ["stats"]
        gsub => [
            "logdate", "\b(?<val>\d)\b", "0\k<val>",
            "logdate", "\s+", " "
        ]
    }
    date {
        tags => ["stats"]
        match => [ "logdate", "yyyy MM dd HH mm ss" ]
    }

    # do a reverse lookup on the ip...
    dns {
        tags => ["beer"]
        action => "replace"
        reverse => ["client_addr"]
        add_tag => ["dns_reversed"]
    }
    # ... then check if it matches a bot or known "client" software
    grok {
        patterns_dir => "./adslogging/logstash_patterns"
        tags => ["dns_reversed"]
        match => ["client_addr", "%{BOTORCLIENT:bot_or_client}"]
        tag_on_failure => false
    }
    useragent {
        tags => ["apache","access"]
        source => "agent"
        prefix => "useragent__"
    }

    # drop requests for static resources & javascript
    grep { match => ['path', "static"] negate => true }
    grep { match => ['path', "dynjs"] negate => true }

    # maybe turn this on when debugging
    # grep { tags => ['_jsonparsefailure'] drop => true }

    mutate {
        # we've hopefully got what we need and some of the junky values make mongo barf
        remove => ["cookie","agent","qstring"]
    }
}
output {
    stdout { debug => true }
    elasticsearch {
        host => "localhost"
        cluster => "logstash_es"
        node_name => "Node1"
    }
#    gelf { 
#        host => "localhost" 
#        port => 12201 
#        sender => "%{@source}"
#    }
#    mongodb {
#        host => "localhost"
#        database => "logstash"
#        collection => "events"
#        user => "logstash"
#        password => "logstash"
#        isodate => true
#    }
}
